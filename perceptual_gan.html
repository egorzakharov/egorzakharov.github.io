<style type="text/css">
 * { padding: 0; margin: 0;}

    .container {
        padding: 0px;
        margin: 0px;
        color: #404040;
        font-size: 11pt;
        font-family: 'Open Sans', sans-serif;
        font-style: normal;
        font-weight: 400;
        color: #444;
        /*background-color: #FFFDFA;*/
        /*background: linear-gradient(to right, white, #eee, white);
        background: -webkit-linear-gradient( right, white, #eee, white)*/
        /*background-image: url("/PaintSwatches_1400x900.jpg");*/
        /*font-family: 'textbook', arial, sans-serif;*/
        /*margin-left: auto;
        margin-right: auto;*/
        /*max-width: 2000px;*/
    }


   .section {
      padding-top: 20px;
      padding-bottom: 20px;
    }


   .title {
      color: #000000;
      font-family: 'Roboto Condensed', sans-serif;
      padding-top: 10px;
      padding-bottom: 25px;
      font-size: 36pt;
      line-height: 100%;
      font-weight: 700;
      color: #444;
    }
    
    .author {
      font-size: 13pt;
    }

    .section_title {
      color: #000000;
      font-family: 'Roboto Condensed', sans-serif;
      /*padding-top: 10px;*/
      /*padding-bottom: 10px;*/
      font-size: 24pt;
      line-height: 100%;
      font-weight: 700;
      padding-bottom: 10px;
      color: #444;
      /*margin-left: auto;
        margin-right: auto;
      /*line-height: 100%;*/
      /*font-weight: 700;*/
    }
    
  .shadow {
      -webkit-box-shadow: 5px 5px 5px #aaa;
      -moz-box-shadow: 5px 5px 5px #aaa;
      box-shadow: 5px 5px 5px #aaa;
      margin-bottom: 10px;
  }

  .aff {
      margin-bottom: 16px;
      /*max-width: 40px;*/
      max-height: 30px
  }



    .img_authors {
      position: relative;
      max-width: 130px;
      max-height: 130px
    }

    .img_links {
      position: relative;
      max-width: 70px;
      max-height: 70px
    }


.text {
    position:relative;
    line-height:2em;
    overflow:hidden;
}
.fadingEffect {
    position:absolute;
    top:0; bottom:0; right:0;
    width:100%;
    background:#FFFAFA;
    -moz-animation: showHide 5s ease-in alternate infinite; /* Firefox */
    -webkit-animation: showHide 5s ease-in alternate infinite; /* Safari and Chrome */
    -ms-animation: showHide 5s ease-in alternate infinite; /* IE10 */
    -o-animation: showHide 5s ease-in alternate infinite; /* Opera */
    animation: showHide 5s ease-in alternate infinite;
}
@-webkit-keyframes showHide { /* Chrome, Safari */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@-moz-keyframes showHide { /* FF */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@-ms-keyframes showHide { /* IE10 */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@-o-keyframes showHide { /* Opera */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@keyframes showHide {
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}


#cf {
  position:relative;
  max-height:301px;
  /*width:450px;*/

  width:100%;
  height:100%;
  /*overflow:hidden; */
  /*width:inherit;*/

  /*max-height: auto;*/

  /*margin:0 auto;*/
}

#cf img {
  position:absolute;
  
  /*height:100%;*/
  /*width:100%;*/

  opacity: 0;
  left:0;
  -webkit-transition: opacity 1s ease-in-out;
  -moz-transition: opacity 1s ease-in-out;
  -o-transition: opacity 1s ease-in-out;
  transition: opacity 1s ease-in-out;
  animation-name: cfFadeInOut;
  animation-timing-function: ease-in-out;
  animation-iteration-count: infinite;
  animation-duration: 28s;  /* 7*4 */
}


@keyframes cfFadeInOut {
  /* fade out */
  0% {
    opacity:0;
  }
  5% {
    opacity:1;
  }
  /* fade in */
  14.28% {
    opacity:1;
  }
  19.28% {
    opacity:0;
  }
}

#cf img:nth-of-type(7) {
  animation-delay: 24s;
}
#cf img:nth-of-type(6) {
  animation-delay: 20s;
}
#cf img:nth-of-type(5) {
  animation-delay: 16s;
}
#cf img:nth-of-type(4) {
  animation-delay: 12s;
}
#cf img:nth-of-type(3) {
  animation-delay: 8s;
}
#cf img:nth-of-type(2) {
  animation-delay: 4s;
}
#cf img:nth-of-type(1) {
  animation-delay: 0s;
}

.row-centered {
    text-align:center;
}
.col-centered {
    display:inline-block;
    float:none !important;
    /* reset the text-align */
    text-align:center;
    /* inline-block space fix */
    margin-right:-4px;
    min-width: 170px;
}

html, body {
  margin-top: 0px;
  margin-bottom: 0px;
  margin-left: 10px;
  margin-right: 10px;
  height: 100%;             /* need for iframe height 100% to work */
}

iframe {
  box-sizing: border-box;   /* make the border size be included in the height */
  /*display: block;            make them block to fix white space margin */
  width: 100%;
}

.hint {
  color: #A00;
  font-size: 10pt;
  margin-bottom: 10px;
}

.img-process {
  /*width: 100%;*/
  margin: 5px;
  max-height: 200px; 
  -webkit-transition: opacity 0.2s ease-in-out;
  -moz-transition: opacity 0.2s ease-in-out;
  -ms-transition: opacity 0.2s ease-in-out;
  -o-transition: opacity 0.2s ease-in-out;
  transition: opacity 0.2s ease-in-out;
  opacity: 1;
}
.img-process:hover{
  cursor: pointer;
  opacity: 0.6;
}

.centeredd {
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  -webkit-transform: translate(-50%, -50%);
  -moz-transform: translate(-50%, -50%);
  -o-transform: translate(-50%, -50%);
  -ms-transform: translate(-50%, -50%);
}

</style>

<!doctype html>
<html lang="en">
  <head>
      <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <title>Image Manipulation with Perceptual Discriminators</title>
      <meta property="og:title" content="Image Manipulation with Perceptual Discriminators" />

      <!-- Latest compiled and minified CSS -->
      <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

      <!-- Optional theme -->
      <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous"><!-- <link rel="stylesheet" href="../css/bootstrap-theme.min.css"> -->

      <!-- Google fonts -->
      <!-- <link href="../css/google-fonts.css" rel="stylesheet" type="text/css"> -->
      <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,700|Open+Sans:300italic,400italic,600italic,400,700,300,600" rel="stylesheet" type="text/css">

      <!-- Photoswipe -->
      <link href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.css" rel="stylesheet">
      <link href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.css" rel="stylesheet">

      <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>

    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  <script>
  function resizeIframe(obj) {
    obj.style.height = 0;
    obj.style.height = (obj.contentWindow.document.body.scrollHeight + 5) + 'px';
  }




  
</script>
    <script language="JavaScript">

      function resize_iframe()
      {
        obj = document.getElementById("myiframe")

        obj.style.height = 0;
        obj.style.height = (obj.contentWindow.document.body.scrollHeight) + 'px';
        
        document.getElementById("cf").style.height = document.getElementById("lastimage").height + 8;
      }
      window.onresize=resize_iframe; 
      

    </script>

  </head>

  <body>
    <script src="zepto.min.js"></script>
  <script>
    window.addEventListener('message', function(event) {
      if(height = event.data['height']) {
        $('iframe').css('height', height + 'px')
      }
    });
    

    // var resizeEvent = new Event('resize');
    // window.dispatchEvent(resizeEvent);
    
  </script>
  <div class="container">

  <center>
    <div class="title">Image Manipulation with Perceptual
Discriminators</div>
  </center>
   

 <center>
      <div class="row row-eq-height" style="margin-bottom: -10px">
        <!-- <div class="col-sm-12"> -->
        <div class="col-sm-15">
          <a href="https://faculty.skoltech.ru/people/dianasungatullina">
            <div class="col-sm-3" style="margin-bottom: 10px">
              <!-- <div class="col-sm-12"> -->
                <div class="author"><span style="font-weight: 600">Diana Sungatullina*</span><br>Skoltech</div>
              <!-- </div> -->
            </div>
          </a>
          <a href="https://github.com/egorzakharov">
            <div class="col-sm-3" style="margin-bottom: 10px">
              <!-- <div class="col-sm-12"> -->
                <div class="author"><span style="font-weight: 600">Egor Zakharov*</span><br>Skoltech</div>
              <!-- </div> -->
            </div>
          </a>
          <a href="https://dmitryulyanov.github.io/about">
            <div class="col-sm-3" style="margin-bottom: 10px">
              <!-- <div class="col-sm-12"> -->
                <div class="author"><span style="font-weight: 600">Dmitry Ulyanov</span><br>Skoltech</div>
              <!-- </div> -->
            </div>
          </a>
          <a href="http://sites.skoltech.ru/compvision/members/vilem/">
            <div class="col-sm-3" style="margin-bottom: 10px">
              <!-- <div class="col-sm-12"> -->
                <div class="author"><span style="font-weight: 600">Victor Lempitsky</span><br>Skoltech, Samsung</div>
              <!-- </div> -->
            </div>
          </a>
          <br>
          <div>
              <span style="font-size:14px"><i>*Equal contribution</i>
          </div>
        </div>
      <!-- </div> -->
    </div>
    </center>


  <!-- ====================================================== -->
  <!-- ===================== TEASER ============== -->
  <!-- ====================================================== -->

  <center>
    <div class="row section center row-eq-height">
      <div class="col center col-sm-12">
          <center>
          <img src="assets/results_256p.jpg" class="img-responsive"/>
          </center>
        <span style="font-size:20px"><i>Example results for facial attribute editing</i>
        </span>
      </div>
    </div>
  </center>





    <!-- ====================================================== -->
    <!-- ===================== ABSTRACT ======================= -->
    <!-- ====================================================== -->
    <div class="row section">
      <!-- <div class="col-xs-8 col-xs-offset-2 text-justify"> -->
      <div class="section_title">Abstract</div>
      <p> <!-- style="text-indent: 30px" -->
        Systems that perform image manipulation using deep convolutional networks have achieved remarkable realism. Perceptual losses and losses based on adversarial discriminators are the two main classes of learning objectives behind these advances. In this work, we show how these two ideas can be combined in a principled and non-additive manner for unaligned image translation tasks. This is accomplished through a special architecture of the discriminator network inside generative adversarial learning framework. The new architecture, that we call a \textit{perceptual discriminator}, embeds the convolutional parts of a pre-trained deep classification network inside the discriminator network. The resulting architecture can be trained on unaligned image datasets, while benefiting from the robustness and efficiency of perceptual losses. We demonstrate the merits of the new architecture in a series of qualitative and quantitative comparisons with baseline approaches and state-of-the-art frameworks for unaligned image translation.
      </p>
      <!-- </div> -->
    </div>
    
  <center>
    <div class="row section" style="margin-bottom: -12px">
      <div class="col-sm-8">
      <!-- <div class="col-sm-12 center"> -->
        <div class="col-sm-4" style="margin-bottom: 12px">
          <!-- <div class="col-sm-12"> -->
            <a href="http://arxiv.org/abs/1809.01396">
                <img src="assets/pdf.svg" class="img_links img-responsive">
                <div class="author">Paper</div>
            </a>
          <!-- </div> -->
        </div>
      <!-- </div> -->
    </div>
    </div>
  </center>

    <div class="row section">
      <!-- <div class="col-xs-8 col-xs-offset-2 text-justify"> -->
      <div class="section_title">Main idea</div>
      <center>
        <div class="row section center row-eq-height">
          <div class="col center col-sm-12">
              <center>
              <img src="assets/perceptual.jpg" class="img-responsive"/>
              </center>
            </span>
          </div>
        </div>
      </center>
      <p> <!-- style="text-indent: 30px" -->
        Generative adversarial networks have shown impressive results in photorealistic image synthesis. The model includes a generative network $G$ trained to produce samples $y \sim p_\text{fake}(y)$ that match target distribution $p_\text{real}(y)$ in the data space $\mathcal Y$, and a discriminator network $D$ that is trained to distinguish whether the input is real or generated by $G$. The two networks optimize the policy function $V(D, G)$:
        
        <!-- <center>
          <span style="width:100%; height: auto; display: block;padding: 0;margin: 0;"> -->
          \begin{equation*}
              \min_{G} \max_{D}\ V(D, G)
          \label{eq:gan}
          \end{equation*}
         <!--  </span>
        </center>  -->

        where $V(D, G)$ is usually chosen so that $\max_{D} V(D, G)$ evaluates to the divergence between distributions $p_\text{real}(y)$ and $p_\text{fake}(y)$.
      </p>
      <p>
        Converging to good equilibria for any of the proposed GAN games is known to be hard. In general, the performance of the trained generator network crucially depends on the architecture of the discriminator network, that needs to learn meaningful statistics, which are good for matching the target distribution $p_\text{real}$. The typical failure mode of GAN training is when the discriminator does not manage to learn such statistics before being ``overpowered'' by the generator. 
      </p>
      <p>
        Following this line of work, we suggest to base the GAN discriminator $D(y)$ on the perceptual statistics computed by the reference network $F$ on the input image $y$, which can be either real (coming from $p_\text{target}$) or fake (produced by the generator). Our motivation is that a discriminator that uses perceptual features has a better chance to learn good statistics than a discriminator initialized to a random network. For simplicity, we assume that the network $F$ has a chain structure, e.g. $F$ can be presented by VGG.
      </p>
      <p>
        Consider the subsequent blocks of the convolutional part of the reference network $F$, and denote them as $b_0,b_1,\dots,b_{K-1}$. Each block may include one or more convolutional layers interleaved with non-linearities and pooling operations. Then, the perceptual statistics $\{f_1(y), \dots, f_K(y)\}$ are computed as:
        \begin{eqnarray*}
        f_1(y) &=& b_0(y)\\
        f_i(y) &=& b_{i-1}(f_{i-1}(y)), \quad i = 2,\dots,K\,,
        \end{eqnarray*}
        so that each $f_i(y)$ is a stack of convolutional maps of the spatial dimensions $W_i \times W_i$. The dimension $W_i$ is determined by the preceeding size $W_{i-1}$ as well as by the presence of strides and pooling operations inside $b_i$. In our experiments we use features from consecutive blocks, i.e. $W_i = W_{i-1} / 2$.
      </p>
      <p>
        Proposed discriminator architecture combines together perceptual statistics using the following computations:
        \begin{eqnarray*}
        h_1(y) &=& f_1(y)\\
        h_i(y) &=& \texttt{stack}\left[ c_{i-1}(h_{i-1}(y),\phi_{i-1})\,,\, f_i(y) \right], \quad i = 2,\dots,K\,, \label{eq:midlayer}
        \end{eqnarray*}
        where stack denotes stacking operation, and the convolutional blocks $c_j$ with learnable parameters $\phi_j$ (for $j = 1,\dots,K-1$) are composed of convolutions, leaky ReLU nonlinearities, and average pooling operations. Each of the $c_j$ blocks thus transforms map stacks of the spatial size $W_j \times W_j$ to map stacks of the spatial size  $W_{j+1} \times W_{j+1}$. Thus, the strides and pooling operations inside $c_j$ match the strides and/or pooling operations inside $b_j$.
      </p>

      <!-- </div> -->
    </div>

    <div class="row section">
      <!-- <div class="col-xs-8 col-xs-offset-2 text-justify"> -->
      <div class="section_title">Experiments</div>
        <p>
            Below are additional experimental results to suplement the paper.
        </p>
        <a style="font-weight: 600" href="assets/faces_attributes.html">More attributes manipulation examples</a><br>
        <a style="font-weight: 600" href="assets/apple_to_orange.html">Apple to orange</a><br>
        <a style="font-weight: 600" href="assets/photos2monet.html">Photos to Monet</a>
    </div>

    <div class="row section">
      <!-- <div class="col-xs-8 col-xs-offset-2 text-justify"> -->
      <div class="section_title">Acknowledgements</div>
      <p>This work has been supported by the Ministry of Education and Science of the Russian Federation (grant 14.756.31.0001).</p>

    </div>
    
    <script src="img-gallery.js"></script>
    <!-- Google Analytics -->
<!--     <script type="text/javascript">
      document.getElementById("cf").style.height = document.getElementById("lastimage").height + 8;
    </script> -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', '{{ site.google_analytics }}', 'auto');
    ga('send', 'pageview', {
      'page': '{{ site.baseurl }}{{ page.url }}',
      'title': '{{ page.title | replace: "'", "\\'" }}'
    });
  </script>
  <!-- End Google Analytics -->
    <!-- Yandex.Metrika counter -->
    <script type="text/javascript">
      (function (d, w, c) {
          (w[c] = w[c] || []).push(function() {
              try {
                  w.yaCounter41551219 = new Ya.Metrika({
                      id:41551219,
                      clickmap:true,
                      trackLinks:true,
                      accurateTrackBounce:true,
                      webvisor:true
                  });
              } catch(e) { }
          });

          var n = d.getElementsByTagName("script")[0],
              s = d.createElement("script"),
              f = function () { n.parentNode.insertBefore(s, n); };
          s.type = "text/javascript";
          s.async = true;
          s.src = "https://mc.yandex.ru/metrika/watch.js";

          if (w.opera == "[object Opera]") {
              d.addEventListener("DOMContentLoaded", f, false);
          } else { f(); }
      })(document, window, "yandex_metrika_callbacks");
  </script>
  <noscript><div><img src="https://mc.yandex.ru/watch/41551219" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

</body>
</html>
 
